{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import sqlite3\n",
    "import spacy\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import MiniBatchKMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load the SpaCy German language model\n",
    "# nlp = spacy.load(\"de_core_news_sm\")\n",
    "nlp = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def german_stopwords_plain():\n",
    "\n",
    "  # Specify the path to your text file\n",
    "  file_path = './german_stopwords_plain.txt'\n",
    "\n",
    "  # Open the file in read mode\n",
    "  with open(file_path, 'r') as file:\n",
    "      # Read the content of the file\n",
    "      lines = file.readlines()\n",
    "\n",
    "      # Filter out lines starting with ';'\n",
    "      filtered_lines = [line.strip() for line in lines if not line.startswith(';')]\n",
    "\n",
    "      # Concatenate the non-comment lines into a single string\n",
    "      content = '\\n'.join(filtered_lines)\n",
    "\n",
    "      # Split the content into words\n",
    "      words = content.split()\n",
    "\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def german_stopwords_full():\n",
    "\n",
    "  # Specify the path to your text file\n",
    "  file_path = './german_stopwords_full.txt'\n",
    "\n",
    "  # Open the file in read mode\n",
    "  with open(file_path, 'r') as file:\n",
    "      # Read the content of the file\n",
    "      lines = file.readlines()\n",
    "\n",
    "      # Filter out lines starting with ';'\n",
    "      filtered_lines = [line.strip() for line in lines if not line.startswith(';')]\n",
    "\n",
    "      # Concatenate the non-comment lines into a single string\n",
    "      content = '\\n'.join(filtered_lines)\n",
    "\n",
    "      # Split the content into words\n",
    "      words = content.split()\n",
    "\n",
    "  return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions - Regulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Clusters - Regulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters_regulated(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=512, batch_size=1024, random_state=500).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - TSNE - Regulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_pca_regulated(data, labels):\n",
    "    max_label = max(labels)\n",
    "    max_items = np.random.choice(range(data.shape[0]), size=10000, replace=False)\n",
    "\n",
    "    data_array = np.asarray(data[max_items, :].todense())\n",
    "\n",
    "    pca = PCA(n_components=2).fit_transform(data_array)\n",
    "    tsne = TSNE(perplexity=50).fit_transform(pca)\n",
    "    \n",
    "    \n",
    "    idx = np.random.choice(range(pca.shape[0]), size=100, replace=False)\n",
    "    # label_subset = labels[max_items]\n",
    "    # label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
    "    label_subset = labels[max_items][idx]\n",
    "    unique_labels = np.unique(label_subset)\n",
    "    colors = [cm.hsv(i/max_label) for i in range(max_label+1)]\n",
    "    \n",
    "    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    for i in unique_labels:\n",
    "        ix = np.where(label_subset == i)\n",
    "        ax[0].scatter(pca[idx, 0][ix], pca[idx, 1][ix], c=[colors[i]], label=i)\n",
    "    ax[0].set_title('PCA Cluster Plot')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    for i in unique_labels:\n",
    "        ix = np.where(label_subset == i)\n",
    "        ax[1].scatter(tsne[idx, 0][ix], tsne[idx, 1][ix], c=[colors[i]], label=i)\n",
    "    ax[1].set_title('TSNE Cluster Plot')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions - Unregulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Clusters - Unregulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters_unregulated(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=1000).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - TSNE - Unregulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_pca_unregulated(data, labels):\n",
    "    max_label = max(labels)\n",
    "    max_items = np.random.choice(range(data.shape[0]), size=60000, replace=False)\n",
    "\n",
    "    data_array = np.asarray(data[max_items, :].todense())\n",
    "\n",
    "    pca = PCA(n_components=2).fit_transform(data_array)\n",
    "    tsne = TSNE(perplexity=50).fit_transform(pca)\n",
    "    \n",
    "    \n",
    "    idx = np.random.choice(range(pca.shape[0]), size=10000, replace=False)\n",
    "    # label_subset = labels[max_items]\n",
    "    # label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
    "    label_subset = labels[max_items][idx]\n",
    "    unique_labels = np.unique(label_subset)\n",
    "    colors = [cm.hsv(i/max_label) for i in range(max_label+1)]\n",
    "    \n",
    "    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    for i in unique_labels:\n",
    "        ix = np.where(label_subset == i)\n",
    "        ax[0].scatter(pca[idx, 0][ix], pca[idx, 1][ix], c=[colors[i]], label=i)\n",
    "    ax[0].set_title('PCA Cluster Plot')\n",
    "    ax[0].legend()\n",
    "    \n",
    "    for i in unique_labels:\n",
    "        ix = np.where(label_subset == i)\n",
    "        ax[1].scatter(tsne[idx, 0][ix], tsne[idx, 1][ix], c=[colors[i]], label=i)\n",
    "    ax[1].set_title('TSNE Cluster Plot')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Connection and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = sqlite3.connect('weiterbildung_new_data.db')\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT angebot_id, angebot_titel, angebot_inhalt, bildungsart_bezeichnung FROM weiterbildung_data\", sqlite_conn)\n",
    "df = df.drop_duplicates(subset=['angebot_id'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Education | bildungsart_bezeichnung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "educationTypes = df.groupby(['bildungsart_bezeichnung']).bildungsart_bezeichnung.value_counts()\n",
    "educationTypes.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Regulated DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regulated = df[df[\"bildungsart_bezeichnung\"].str.contains(\"Gesetzlich/gesetzes√§hnlich geregelte Fortbildung/Qualifizierung\")]\n",
    "df_regulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Unregulated DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unregulated = df[df[\"bildungsart_bezeichnung\"] == \"Fortbildung/Qualifizierung\"]\n",
    "# df_unregulated = df_unregulated.iloc[:8000]\n",
    "df_unregulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lemmatize text using SpaCy\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to the regulated dataset\n",
    "df_regulated['lemmatized_text'] = df_regulated['angebot_inhalt'].apply(lemmatize_text)\n",
    "df_regulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to the unregulated dataset\n",
    "df_unregulated['lemmatized_text'] = df_unregulated['angebot_inhalt'].apply(lemmatize_text)\n",
    "df_unregulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Top Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(data, clusters, labels, n_terms):\n",
    "    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n",
    "    \n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution - Regulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFID Vectorizer - Regulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    min_df = 500,\n",
    "    max_df = 0.95,\n",
    "    max_features = 3000,\n",
    "    stop_words=german_stopwords_full()\n",
    ")\n",
    "\n",
    "text_regulated = tfidf.fit_transform(df_regulated.lemmatized_text)\n",
    "\n",
    "find_optimal_clusters_regulated(text_regulated, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and TSNE  - Regulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_regulated = MiniBatchKMeans(n_clusters=10, n_init='auto', random_state=0).fit(text_regulated)\n",
    "\n",
    "test_regulated = clusters_regulated.predict(text_regulated)\n",
    "\n",
    "plot_tsne_pca_regulated(text_regulated, test_regulated)\n",
    "get_top_keywords(text_regulated, test_regulated, tfidf.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution - Unregulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFID Vectorizer - Unregulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_unregulated = TfidfVectorizer(\n",
    "    min_df = 500,\n",
    "    max_df = 0.95,\n",
    "    max_features = 201,\n",
    "    stop_words=german_stopwords_full()\n",
    ")\n",
    "\n",
    "text_unregulated = tfidf_unregulated.fit_transform(df_unregulated.lemmatized_text)\n",
    "\n",
    "find_optimal_clusters_regulated(text_unregulated, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and TSNE - Unregulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_unregulated = MiniBatchKMeans(n_clusters=8, n_init='auto', random_state=0).fit_predict(text_unregulated)\n",
    "\n",
    "plot_tsne_pca_unregulated(text_unregulated, clusters_unregulated)\n",
    "\n",
    "get_top_keywords(text_unregulated, clusters_unregulated, tfidf_unregulated.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Distancing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_clusters = MiniBatchKMeans(n_clusters=10, n_init='auto', random_state=0).fit(text_regulated)\n",
    "reg_clusters_words = reg_clusters.predict(text_regulated)\n",
    "\n",
    "unreg_clusters = MiniBatchKMeans(n_clusters=8,n_init='auto', random_state=0).fit(text_unregulated)\n",
    "unreg_clusters_words = unreg_clusters.predict(text_unregulated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquiring Cluster Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cluster_center = reg_clusters.cluster_centers_\n",
    "unreg_cluster_center = unreg_clusters.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stats = []\n",
    "\n",
    "for i in range(unreg_cluster_center.shape[0]):\n",
    "  cluster_to_cluster_centroid_distance = []\n",
    "  for j in range(reg_cluster_center.shape[0]):\n",
    "    centroid_distance = euclidean(unreg_cluster_center[i], reg_cluster_center[j])\n",
    "    cluster_to_cluster_centroid_distance.append((i,j,centroid_distance))\n",
    "  min_stats = min(cluster_to_cluster_centroid_distance, key=lambda x: x[2])\n",
    "  final_stats.append(min_stats)\n",
    "final_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Top Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_keywords(text_regulated, reg_clusters_words, tfidf.get_feature_names_out(), 10)\n",
    "get_top_keywords(text_unregulated, unreg_clusters_words, tfidf_unregulated.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - TSNE Plotting (Regualted and Unregulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label_reg = max(reg_clusters_words)\n",
    "max_label_unreg = max(unreg_clusters_words)\n",
    "\n",
    "max_itemsReg = np.random.choice(range(text_regulated.shape[0]), size=10000, replace=False)\n",
    "max_itemsUnreg = np.random.choice(range(text_unregulated.shape[0]), size=1000, replace=False)\n",
    "\n",
    "data_arrayReg = np.asarray(text_regulated[max_itemsReg, :].todense())\n",
    "data_arrayUnreg = np.asarray(text_unregulated[max_itemsUnreg, :].todense())\n",
    "\n",
    "# pca = PCA(n_components=2).fit_transform(data_array)\n",
    "Reg_pca = PCA(n_components=2).fit_transform(data_arrayReg)\n",
    "Unreg_pca = PCA(n_components=2).fit_transform(data_arrayUnreg)\n",
    "Reg_tsne = TSNE(n_components=2, perplexity=50).fit_transform(data_arrayReg)\n",
    "Unreg_tsne = TSNE(n_components=2, perplexity=50).fit_transform(data_arrayUnreg)\n",
    "\n",
    "idx_reg = np.random.choice(range(Reg_pca.shape[0]), size=10000, replace=False)\n",
    "label_subset_reg = reg_clusters_words[max_itemsReg]\n",
    "label_subset_reg = [cm.hsv(i/max_label_reg) for i in label_subset_reg[idx_reg]]\n",
    "\n",
    "idx_unreg = np.random.choice(range(Unreg_pca.shape[0]), size=1000, replace=False)\n",
    "label_subset_unreg = unreg_clusters_words[max_itemsUnreg]\n",
    "label_subset_unreg = [cm.hsv(i/max_label_unreg) for i in label_subset_unreg[idx_unreg]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Function to generate legend items for a selection of clusters\n",
    "def generate_legend_items(num_clusters):\n",
    "    return [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cm.hsv(i/num_clusters), markersize=10, label=f'Cluster {i}') for i in range(num_clusters)]\n",
    "\n",
    "# PCA for dataset X\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(Reg_pca[:, 0], Reg_pca[:, 1], c=label_subset_reg, cmap='viridis', alpha=0.5)\n",
    "plt.scatter(reg_clusters.cluster_centers_[:, 0], reg_clusters.cluster_centers_[:, 1], marker='x', color='red', s=100)\n",
    "plt.title('PCA for Regulated CVET')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(handles=generate_legend_items(max_label_reg + 1))\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(Reg_tsne[:, 0], Reg_tsne[:, 1], c=label_subset_reg, cmap='viridis', alpha=0.5)\n",
    "plt.scatter(reg_clusters.cluster_centers_[:, 0], reg_clusters.cluster_centers_[:, 1], marker='x', color='red', s=100)\n",
    "plt.title('t-SNE for Regulated CVET')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend(handles=generate_legend_items(max_label_reg + 1))\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(Unreg_pca[:, 0], Unreg_pca[:, 1], c=label_subset_unreg, cmap='viridis', alpha=0.5)\n",
    "plt.scatter(unreg_clusters.cluster_centers_[:, 0], unreg_clusters.cluster_centers_[:, 1], marker='x', color='red', s=100)\n",
    "plt.title('PCA for Unegulated CVET')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(handles=generate_legend_items(max_label_reg + 1))\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(Unreg_tsne[:, 0], Unreg_tsne[:, 1], c=label_subset_unreg, cmap='viridis', alpha=0.5)\n",
    "plt.scatter(unreg_clusters.cluster_centers_[:, 0], unreg_clusters.cluster_centers_[:, 1], marker='x', color='red', s=100)\n",
    "plt.title('t-SNE for Unegulated CVET')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend(handles=generate_legend_items(max_label_reg + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
